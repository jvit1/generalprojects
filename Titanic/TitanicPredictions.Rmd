```{r warning=FALSE, include=FALSE}
library(tidyverse)
library(tidymodels)
library(modelr)
library(randomForest)
library(ggplot2)
library(knitr)
library(gridExtra)
library(car)
train <- read.csv("C:/Users/student/Documents/UVA/Portfolio Projects/generalprojects/Titanic/train.csv")
test <- read.csv("C:/Users/student/Documents/UVA/Portfolio Projects/generalprojects/Titanic/test.csv")
```

## Exploratory
Let's take a look into the data.
```{r}
str(train)
```
As we can see, our response variable (survived) isn't currently a factor. This will cause some issues later down the road, so let's address it now as well as making some other general changes. Of course, I'm changing these just based on 'gut' feeling, so I might have to change them back later on.
```{r}
train$Sex <- as.factor(train$Sex)
train$Survived <- as.factor(train$Survived)
train$Pclass <- as.factor(train$Pclass)
train$Embarked <- as.factor(train$Embarked)
train$SibSp <- as.factor(train$SibSp)
train$Parch <- as.factor(train$Parch)


train$Embarked[train$Embarked==""]<-"Any text, NA will be generated"
train$Cabin[train$Cabin==""]<-NA
```

Let's look to see if there's any missing values.
```{r}
colSums(is.na(train))
```
We've got some to deal with. Let's get into a visual analysis to see if it's worth trying to estimate these missing values.

### Visual
To start, let's take a look at age and sex to see how those play into who lives.
```{r}
#Sex
train %>% select(Survived, Sex) %>%
  ggplot(aes(x=Sex, fill = Survived)) + geom_bar(position='dodge')
```
It's clear to see that women survive at higher rates than men do.

```{r}
#Age
train %>% select(Survived, Age) %>%
  ggplot() + geom_histogram(aes(x=Age, fill = Survived))
```
Based just on the graphic, it's pretty tough to tell if Age is a predictor. We can try to take a look at economic status based on ticket prices.

```{r}
#Ticket Price
low.fare <- train %>% select(Survived, Fare) %>% filter(Fare < 100) %>%
  ggplot() + geom_histogram(aes(x=Fare, fill = Survived)) +theme(legend.position = 'none')

high.fare <-train %>% select(Survived, Fare) %>% filter(Fare > 100) %>%
  ggplot() + geom_histogram(aes(x=Fare, fill = Survived))

grid.arrange(low.fare, high.fare, ncol=2)
```
It's important to note the scales of each graph, but it's easy to see that low priced fares have lower survival rates. With that being said, let's look to see if where people embark from could make a difference.

```{r}
#Embark
train %>% select(Survived, Embarked) %>% drop_na() %>%
  ggplot(aes(x=Embarked, fill = Survived)) + geom_bar(position='dodge') 
```
Based on this graphic, it looks like a whole lot more people left from S. Just out of curiosity, let's look into some demographic information. Using both embarked and ticket price may end up overfitting the model, so this step may have some important results.

```{r}
train %>% select(Fare, Embarked)%>% 
  group_by(Embarked) %>% 
  drop_na() %>% summarise('Median Fare' = median(Fare)) 
```
This pairs up with the graph above. More people survived than died when embarking from C, and C also has the highest median fare.

```{r}
train %>% select(Sex, Survived, Age) %>% drop_na() %>%
  group_by(Survived, Sex) %>% summarise('Median Age' = median(Age))
```
Here's another interesting conclusion. The median survival age for females is lower compared to those that did not survive. This is the opposite in males. Let's break this down further into the ticket class (pclass).

```{r}
train %>% select(Sex, Survived, Age, Pclass) %>% drop_na() %>%
  group_by(Survived, Pclass) %>% summarise('Median Age' = median(Age))
```
To no surprise, passengers in the highest ticket class had the highest median age. This could lead to the conclusion that age may not be a significant predictor of survived, and ticket class may be more effective. Let's begin building a simple logistic model to start.

## Logistic Model
Before we start, we have to take care of some of the missing values. Here's a reminder of what we are missing:
```{r}
colSums(is.na(train))
```

For now, we'll try out inputting the median age of 28, which is found below.
```{r}
medianAge <- train %>% select(Age) %>% summarise(median=median(Age, na.rm=TRUE))
train$Age[is.na(train$Age)] <- medianAge
```

Now we'll build a couple logistic models. 
```{r}
logistic.train <- train
logistic.train$Age <- as.numeric(logistic.train$Age)

#Changing male to 0 and female to 1
logistic.train <- logistic.train %>% 
  mutate(Sex = ifelse(Sex == 'male',0,1))


logistic.train$Sex <- as.numeric(logistic.train$Sex)
logistic.train$Parch <- as.numeric(logistic.train$Parch)
logistic.train$SibSp <- as.numeric(logistic.train$SibSp)


str(logistic.train)

log.mod1 <- glm(Survived ~ Sex + Age + SibSp + Parch + Fare, family="binomial", data=logistic.train)
summary(log.mod1)
vif(log.mod1)
```

All in all, the logistic regression model using Sex, Age, SibSp, Parch, and Fare is not too bad. Let's apply these predictions to the data to see how we did.

```{r}
logistic.train1 <- logistic.train %>%
  add_predictions(log.mod1, type = "response") %>%
  mutate(pred = ifelse(pred >= .5, 1,0))
```

Checking confusion matrix and accuracy rate:
```{r}
logistic.train1$pred <- as.factor(logistic.train1$pred)

logistic.train1 %>%
  conf_mat(truth = Survived, estimate = pred)

## Accuracy rate
logistic.train1 %>%
  metrics(truth = Survived, estimate = pred) %>%
  filter(.metric == "accuracy")
```
Not the best model at all. Let's explore random forests using the same predictors to see if that will make a difference.

```{r}
train.clean <- train %>% select(-Name, -PassengerId)
RF.1 <- randomForest(Survived ~ Sex + Age + SibSp + Parch + Fare , data = train.clean,
                     mtry = 2, importance= TRUE)

RF.1 %>% importance
```

Now we'll go head and add these to the data then assess the accuracy.
```{r}
#Adding to data
RF.add1 <- train.clean %>%
  add_predictions(RF.1, type = "response") %>%
  mutate(pred = ifelse(pred >= .5, 1,0))


RF.add1$Survived <- as.factor(RF.add1$Survived)
RF.add1$pred <- as.factor(RF.add1$pred)


#Assessments
RF.add1 %>%
  conf_mat(truth = Survived, estimate = pred)

RF.add1 %>%
  metrics(truth = Survived, estimate = pred) %>%
  filter(.metric == "accuracy")
```
Just as expected, random forest seems like the model to use. Let's do some more exploration and apply the final model to the testing data and submit. 

Let's see which values we need to fill.
```{r}
colSums(is.na(test))
```
Looks like we'll have to estimate some ages. Based on our EDA from above, we will input the median age depending on where the passenger embarked. This is done to get a slightly better median age which will aid our model more than using the median total age. The same will be done for the missing fare value.

```{r}
#Age
test.ages <- test %>% select(Embarked, Age) %>% 
  group_by(Embarked) %>% 
  drop_na() %>% 
  summarise("Median Age" = median(Age))

C.age <- test.ages$`Median Age`[1]
Q.age <- test.ages$`Median Age`[2]
S.age <- test.ages$`Median Age`[3]


test$Age <- ifelse(is.na(test$Age) & test$Embarked == 'C', C.age, test$Age)
test$Age <- ifelse(is.na(test$Age) & test$Embarked == 'Q', Q.age, test$Age)
test$Age <- ifelse(is.na(test$Age) & test$Embarked == 'S', S.age, test$Age)

#Fare
which(is.na(test$Fare))
test[153,]  #Left from S, let's use that median

test.fare <- test %>% filter(Embarked == 'S') %>% drop_na() %>% summarise("Median" = median(Fare))
S.fare <- test.fare[1]

test$Fare <- ifelse(is.na(test$Fare) & test$Embarked == 'S', C.age, test$Fare)
```
Boom. Problem solved. Let's apply this to the testing and submit.

```{r}
RF.test <- test %>%
  add_predictions(RF.1) %>%
  rename(pred_pct = pred) %>%
  mutate(method = "RF.1") %>%
  mutate(prediction = ifelse(pred_pct > .5, 1,0))

final.results <- RF.test %>%
  select(PassengerId, prediction) %>%
  rename(Survived = prediction)

write.csv(final.results, 'C:/Users/student/Documents/UVA/Portfolio Projects/generalprojects/Titanic/results.csv' , row.names=FALSE)
```

